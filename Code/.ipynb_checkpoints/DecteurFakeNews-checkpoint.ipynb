{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Objectif:\n",
    "    Le but du projet est de créer un programme qui distingue les TrueNews et des FakeNews, le problème est donc un classification.\n",
    "On a accès a plusieurs paramettre d'entrée mais on simplifira au maximum pour minimiser le temps de calcule. On a donc en entrée le Titre et le Texte et en sortie 0 (Fake) ou 1 (True).\n",
    "\n",
    "Données:\n",
    "    On prend un grand nombre de données (50 000 News), pour une grande précision, avec chacune un Titre, un Texte et 0 si il est faux ou 1 si il est vrai, donc uniquement les attribus les plus pertinnent à étudieer pour dire si une information est vrai ou non.\n",
    "Pour améliorer la qualité, nous utiliseront 2 bases de données distinctes regroupées dans un même format.\n",
    "Dans les bases que nous avons pris, il n'y a pas de donnée manquantes, mais si il y en avait eu une, on l'aurait simplement supprimé: le base de donnée est suffisement grande pour ne pas que ça impacte le résultat final.\n",
    "\n",
    "Choix du métriques: accuracy (précision) car on est dans un problème de classification. Les résultats obtenus sont  suffisement satisfaisant (plus de 95% de réussite) pour ne pas avoir besoin d'essayer d'autres métriques."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialisation DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finish Cell\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "tsize=0.25 #test size pour les train_test_split\n",
    "g=0.1 #Gamma pour la classification avec SVN\n",
    "\n",
    "chemin1=\"../Doc/fake_or_real_news.csv\"\n",
    "chemin2F = \"../Doc/Fake.csv\"\n",
    "chemin2T = \"../Doc/True.csv\"\n",
    "\n",
    "#Traitement de la 1ere BdD\n",
    "dt=pd.read_csv(chemin1)\n",
    "dt=dt.drop(['Unnamed: 0'], axis=1)\n",
    "\n",
    "#Traitement de la 2nd BdD\n",
    "dttmpF = pd.read_csv(chemin2F)\n",
    "dttmpT = pd.read_csv(chemin2T)\n",
    "\n",
    "dttmpF[\"label\"] = \"0\"\n",
    "dttmpT[\"label\"] = \"1\"\n",
    "\n",
    "dttmp = pd.concat([dttmpF,dttmpT])\n",
    "dttmp = dttmp.drop(['subject', 'date'], axis=1)\n",
    "\n",
    "for i in range(dt.shape[0]):\n",
    "    if dt[\"label\"][i] == \"FAKE\":\n",
    "        dt[\"label\"][i] =0;\n",
    "    else:\n",
    "        dt[\"label\"][i] =1;\n",
    "\n",
    "#Envisager de mélanger les lignes si besoin.\n",
    "\n",
    "#Mise en commun des 2 BdD\n",
    "dt = pd.concat([dt,dttmp])\n",
    "print(\"Finish Cell\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finish Cell\n"
     ]
    }
   ],
   "source": [
    "#Nettoyage de la dt\n",
    "import string as st\n",
    "\n",
    "def net_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub('\\n',' ',text)# On retire les caractères spéciaux\n",
    "    text = \"\".join([ch for ch in text if ch not in st.punctuation]) # Retire le ponctuation\n",
    "    text = re.split('\\s+', text) #On sépare chaque mot pour appliquer en liste la suite\n",
    "    text = [x for x in text if len(x)>2] # On retire les mots de 2 lettres, trop présents et sans grande utilité.\n",
    "    text = \" \".join([word for word in text]) #On recolle tout\n",
    "    return text\n",
    "\n",
    "dt['text']=dt['text'].apply(net_text)\n",
    "dt['title']=dt['title'].apply(net_text)\n",
    "\n",
    "\n",
    "print(dt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Juste le texte"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finish Cell\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "#On a besoin d'un nombre donc on va convertir le text en vecteur;\n",
    "\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "X = vectorizer.fit_transform(dt['text'].astype(str))\n",
    "\n",
    "y=dt['label']\n",
    "\n",
    "Xtrain, Xtest, ytrain, ytest = train_test_split(X,y,test_size=tsize)\n",
    "print(\"Finish Cell\")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Classification avec Arbre de décision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9628386290889218\n",
      "[[6361  163]\n",
      " [ 313 5972]]\n",
      "Wall time: 26.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn import metrics\n",
    "\n",
    "Arbre_decision = DecisionTreeClassifier(random_state=0, max_depth=20)\n",
    "clf = Arbre_decision.fit(Xtrain, ytrain.astype(float))\n",
    "\n",
    "ypredit = clf.predict(Xtest)\n",
    "\n",
    "ypredit = pd.Series(ypredit)\n",
    "ytest=ytest.astype(float); #pour avoir le même type\n",
    "\n",
    "print(accuracy_score(ytest, ypredit))\n",
    "print(metrics.confusion_matrix(ytest, ypredit))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Classification avec le plus proche voisin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6482160980560543\n",
      "[[6404  120]\n",
      " [4386 1899]]\n",
      "Wall time: 42.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn import metrics\n",
    "\n",
    "KNN = KNeighborsClassifier()\n",
    "clf = KNN .fit(Xtrain, ytrain.astype(float))\n",
    "ypredit = clf.predict(Xtest)\n",
    "\n",
    "ypredit = pd.Series(ypredit)\n",
    "ytest=ytest.astype(float); #pour avoir le même type\n",
    "\n",
    "print(accuracy_score(ytest, ypredit))\n",
    "print(metrics.confusion_matrix(ytest, ypredit))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Classification avec SVM: avec 50 000 données, le temps de calcule est trop long pour etre envisagé comme solution au problème."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Juste le titre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "#On a besoin d'un nombre donc on va convertir le text en vecteur;\n",
    "\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "X = vectorizer.fit_transform(dt['title'].astype(str))\n",
    "\n",
    "y=dt['label']\n",
    "\n",
    "Xtrain, Xtest, ytrain, ytest = train_test_split(X,y,test_size=tsize)\n",
    "print(\"Finish Cell\")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Classification avec Arbre de décision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn import metrics\n",
    "\n",
    "Arbre_decision = DecisionTreeClassifier(random_state=0, max_depth=20)\n",
    "clf = Arbre_decision.fit(Xtrain, ytrain.astype(float))\n",
    "\n",
    "ypredit = clf.predict(Xtest)\n",
    "\n",
    "ypredit = pd.Series(ypredit)\n",
    "ytest=ytest.astype(float); #pour avoir le même type\n",
    "\n",
    "print(accuracy_score(ytest, ypredit))\n",
    "print(metrics.confusion_matrix(ytest, ypredit))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Classification avec le plus proche voisin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn import metrics\n",
    "\n",
    "KNN = KNeighborsClassifier()\n",
    "clf = KNN .fit(Xtrain, ytrain.astype(float))\n",
    "ypredit = clf.predict(Xtest)\n",
    "\n",
    "ypredit = pd.Series(ypredit)\n",
    "ytest=ytest.astype(float); #pour avoir le même type\n",
    "\n",
    "print(accuracy_score(ytest, ypredit))\n",
    "print(metrics.confusion_matrix(ytest, ypredit))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Titre + texte"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "#On a besoin d'un nombre donc on va convertir le text en vecteur;\n",
    "\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "Xtmp=dt['title'] + dt['text']\n",
    "X = vectorizer.fit_transform(Xtmp);\n",
    "\n",
    "y=dt['label']\n",
    "\n",
    "Xtrain, Xtest, ytrain, ytest = train_test_split(X,y,test_size=tsize)\n",
    "print(\"Finish Cell\")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Classification avec Arbre de décision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn import metrics\n",
    "\n",
    "Arbre_decision = DecisionTreeClassifier(random_state=0, max_depth=20)\n",
    "clf = Arbre_decision.fit(Xtrain, ytrain.astype(float))\n",
    "\n",
    "ypredit = clf.predict(Xtest)\n",
    "\n",
    "ypredit = pd.Series(ypredit)\n",
    "ytest=ytest.astype(float); #pour avoir le même type\n",
    "\n",
    "print(accuracy_score(ytest, ypredit))\n",
    "print(metrics.confusion_matrix(ytest, ypredit))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Classification avec le plus proche voisin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn import metrics\n",
    "\n",
    "KNN = KNeighborsClassifier()\n",
    "clf = KNN .fit(Xtrain, ytrain.astype(float))\n",
    "ypredit = clf.predict(Xtest)\n",
    "\n",
    "ypredit = pd.Series(ypredit)\n",
    "ytest=ytest.astype(float); #pour avoir le même type\n",
    "\n",
    "print(accuracy_score(ytest, ypredit))\n",
    "print(metrics.confusion_matrix(ytest, ypredit))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
